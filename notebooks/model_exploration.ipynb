{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weenkus/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word in stops_words:\n",
    "            sentence.remove(word)  \n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_dataframe(data):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    data = data.dropna(how=\"any\")\n",
    "    \n",
    "    for col in ['question1', 'question2']:\n",
    "        data[col] = data[col].apply(clean_sentence)\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_clean = clean_dataframe(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import gc\n",
    "\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "from simhash import Simhash\n",
    "\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    return data.apply(apply_func, axis=1, raw=True)\n",
    "\n",
    "def chunk(data, num):   \n",
    "    chunk_size = math.ceil(len(data) / num)\n",
    "    return [data[i*chunk_size : (i+1)*chunk_size] for i in range(num)]\n",
    "\n",
    "def pool_apply(data, proc_num=8):\n",
    "    \n",
    "    with Pool(processes=proc_num) as pool:\n",
    "        chunks = chunk(data, proc_num) \n",
    "        proccessed_chunks = list(pool.map(transform_data, chunks))\n",
    "  \n",
    "    return np.hstack(tuple(proccessed_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word_match_share(row):\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    \n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q1words[word] = 1\n",
    "            \n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q2words[word] = 1\n",
    "            \n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row):\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    \n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q1words[word] = 1\n",
    "            \n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops_words:\n",
    "            q2words[word] = 1\n",
    "            \n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    with np.errstate(invalid='ignore'):\n",
    "        shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [\n",
    "            weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "        \n",
    "        total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "\n",
    "        R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "\n",
    "    return R if not math.isnan(R) else 0\n",
    "\n",
    "\n",
    "def start_with_same_first_word(row):\n",
    "    if not isinstance(row['question1'], str) or not isinstance(row['question2'], str):\n",
    "        return 0\n",
    "    \n",
    "    first_word_q1 = row['question1'].split()[0].lower()\n",
    "    first_word_q2 = row['question2'].split()[0].lower()\n",
    "    \n",
    "    return 1 if first_word_q1 == first_word_q2 else 0\n",
    "\n",
    "def question_length(row):\n",
    "    question = row[feature]\n",
    "    return len(question) if isinstance(question, str) else 0\n",
    "\n",
    "def word_count(row):\n",
    "    question = row[feature]\n",
    "    return len(question.split()) if isinstance(question, str) else 0\n",
    "\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "def simhash_distance_seq(row):\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    if not isinstance(q1, str) or not isinstance(q2, str):\n",
    "        return 0\n",
    "\n",
    "    return Simhash(q1).distance(Simhash(q2))\n",
    "\n",
    "def simhash_distance_shingle(row):\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    if not isinstance(q1, str) or not isinstance(q2, str):\n",
    "        return 0\n",
    "    \n",
    "    q1_shingles = get_singles(q1)\n",
    "    q2_shingles = get_singles(q2)\n",
    "    \n",
    "    return Simhash(q1_shingles).distance(Simhash(q2_shingles))\n",
    "\n",
    "def get_singles(sequence, width = 3):\n",
    "    sequence = sequence.lower()\n",
    "    sequence = re.sub(r'[^\\w]+', '', sequence)\n",
    "    return [sequence[i:i + width] for i in range(max(len(sequence) - width + 1, 1))]\n",
    "\n",
    "def get_unigrams(row):\n",
    "    question = str(row[feature])\n",
    "    return [word for word in nltk.word_tokenize(question.lower()) if word not in stops]\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n",
    "\n",
    "def get_common_unigram_ratio(row):\n",
    "    return float(row[\"zunigrams_common_count\"]) / max(len(\n",
    "            set(row[\"unigrams_ques1\"]).union(set(row[\"unigrams_ques2\"])) ),1)\n",
    "\n",
    "def get_bigrams(row):\n",
    "    question = str(row[feature])\n",
    "    return [i for i in nltk.ngrams(question, 2)]\n",
    "\n",
    "def get_common_bigrams(row):\n",
    "    return len( set(row[\"bigrams_ques1\"]).intersection(set(row[\"bigrams_ques2\"])) )\n",
    "\n",
    "def get_common_bigram_ratio(row):\n",
    "    return float(row[\"zbigrams_common_count\"]) / max(len(\n",
    "            set(row[\"bigrams_ques1\"]).union(set(row[\"bigrams_ques2\"])) ),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eps = 5000 \n",
    "train_qs = pd.Series(train['question1'].tolist() + train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What would a Trump presidency mean for current...</td>\n",
       "      <td>How will a Trump presidency affect the student...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What does manipulation mean?</td>\n",
       "      <td>What does manipulation means?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Why are so many Quora users posting questions ...</td>\n",
       "      <td>Why do people ask Quora questions which can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Why do rockets look white?</td>\n",
       "      <td>Why are rockets and boosters painted white?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>How should I prepare for CA final law?</td>\n",
       "      <td>How one should know that he/she completely pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What are some special cares for someone with a...</td>\n",
       "      <td>How can I keep my nose from getting stuffy at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What Game of Thrones villain would be the most...</td>\n",
       "      <td>What Game of Thrones villain would you most li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>How do we prepare for UPSC?</td>\n",
       "      <td>How do I prepare for civil service?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>What are some examples of products that can be...</td>\n",
       "      <td>What are some of the products made from crude ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>How do I make friends.</td>\n",
       "      <td>How to make friends ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Is Career Launcher good for RBI Grade B prepar...</td>\n",
       "      <td>How is career launcher online program for RBI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Will a Blu Ray play on a regular DVD player? I...</td>\n",
       "      <td>How can you play a Blu Ray DVD on a regular DV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>What is the best/most memorable thing you've e...</td>\n",
       "      <td>What is the most delicious dish you've ever ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>I was suddenly logged off Gmail. I can't remem...</td>\n",
       "      <td>I can't remember my Gmail password or my recov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>How is the new Harry Potter book 'Harry Potter...</td>\n",
       "      <td>How bad is the new book by J.K Rowling?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question1  \\\n",
       "5   Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "7                      How can I be a good geologist?   \n",
       "11        How do I read and find my YouTube comments?   \n",
       "12               What can make Physics easy to learn?   \n",
       "13        What was your first sexual experience like?   \n",
       "15  What would a Trump presidency mean for current...   \n",
       "16                       What does manipulation mean?   \n",
       "18  Why are so many Quora users posting questions ...   \n",
       "20                         Why do rockets look white?   \n",
       "29             How should I prepare for CA final law?   \n",
       "31  What are some special cares for someone with a...   \n",
       "32  What Game of Thrones villain would be the most...   \n",
       "38                        How do we prepare for UPSC?   \n",
       "48  What are some examples of products that can be...   \n",
       "49                             How do I make friends.   \n",
       "50  Is Career Launcher good for RBI Grade B prepar...   \n",
       "51  Will a Blu Ray play on a regular DVD player? I...   \n",
       "53  What is the best/most memorable thing you've e...   \n",
       "58  I was suddenly logged off Gmail. I can't remem...   \n",
       "62  How is the new Harry Potter book 'Harry Potter...   \n",
       "\n",
       "                                            question2  \n",
       "5   I'm a triple Capricorn (Sun, Moon and ascendan...  \n",
       "7           What should I do to be a great geologist?  \n",
       "11             How can I see all my Youtube comments?  \n",
       "12            How can you make physics easy to learn?  \n",
       "13             What was your first sexual experience?  \n",
       "15  How will a Trump presidency affect the student...  \n",
       "16                      What does manipulation means?  \n",
       "18  Why do people ask Quora questions which can be...  \n",
       "20        Why are rockets and boosters painted white?  \n",
       "29  How one should know that he/she completely pre...  \n",
       "31  How can I keep my nose from getting stuffy at ...  \n",
       "32  What Game of Thrones villain would you most li...  \n",
       "38                How do I prepare for civil service?  \n",
       "48  What are some of the products made from crude ...  \n",
       "49                              How to make friends ?  \n",
       "50  How is career launcher online program for RBI ...  \n",
       "51  How can you play a Blu Ray DVD on a regular DV...  \n",
       "53  What is the most delicious dish you've ever ea...  \n",
       "58  I can't remember my Gmail password or my recov...  \n",
       "62            How bad is the new book by J.K Rowling?  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['is_duplicate']==1][:20][['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply_func always MUST be defined above pool_apply, cheers!    \n",
    "apply_func = word_match_share\n",
    "train['word_share'] = pool_apply(train)\n",
    "\n",
    "apply_func = start_with_same_first_word\n",
    "train['start_with_same_world'] = pool_apply(train)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = question_length\n",
    "train['q1_char_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = question_length\n",
    "train['q2_char_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = word_count\n",
    "train['q1_word_num'] = pool_apply(train)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = word_count\n",
    "train['q2_word_num'] = pool_apply(train)\n",
    "\n",
    "apply_func = tfidf_word_match_share\n",
    "train['rfidf_share'] = pool_apply(train)\n",
    "\n",
    "train['char_difference'] = abs(train['q1_char_num'] - train['q2_char_num'])\n",
    "train['word_difference'] = abs(train['q1_word_num'] - train['q2_word_num'])\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "train['seq_simhash_distance'] = pool_apply(train)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "train['shingle_simhash_distance'] = pool_apply(train)\n",
    "\n",
    "train['avg_word_len_q1'] = train['q1_char_num'] / (train['q1_word_num'] + 10e-4)\n",
    "train['avg_word_len_q2'] = train['q2_char_num'] / (train['q2_word_num'] + 10e-4)\n",
    "train['avg_word_difference'] = abs(train['avg_word_len_q1'] - train['avg_word_len_q2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature = 'question1'\n",
    "# apply_func = get_unigrams\n",
    "# train[\"unigrams_ques1\"] = pool_apply(train)\n",
    "\n",
    "# feature = 'question2'\n",
    "# apply_func = get_unigrams\n",
    "# train[\"unigrams_ques2\"] = pool_apply(train)\n",
    "\n",
    "# apply_func = get_common_unigrams\n",
    "# train['unigrams_common_count'] = pool_apply(train)\n",
    "\n",
    "# apply_func = get_common_unigram_ratio\n",
    "# train['unigrams_common_ratio'] = pool_apply(train)\n",
    "\n",
    "# feature = 'question1'\n",
    "# apply_func = get_bigrams\n",
    "# train[\"bigrams_ques1\"] = pool_apply(train, proc_num=4)\n",
    "\n",
    "# feature = 'question2'\n",
    "# apply_func = get_bigrams\n",
    "# train[\"bigrams_ques2\"] = pool_apply(train, proc_num=4)\n",
    "\n",
    "# apply_func = get_common_bigrams\n",
    "# train['bigrams_common_count'] = pool_apply(train)\n",
    "\n",
    "# apply_func = get_common_bigram_ratio\n",
    "# train['bigrams_common_ratio'] = pool_apply(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "apply_func = start_with_same_first_word\n",
    "test['start_with_same_world'] = pool_apply(test)\n",
    "\n",
    "apply_func = word_match_share\n",
    "test['word_share'] = pool_apply(test)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = question_length\n",
    "test['q1_char_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = question_length\n",
    "test['q2_char_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question1'\n",
    "apply_func = word_count\n",
    "test['q1_word_num'] = pool_apply(test)\n",
    "\n",
    "feature = 'question2'\n",
    "apply_func = word_count\n",
    "test['q2_word_num'] = pool_apply(test)\n",
    "\n",
    "apply_func = tfidf_word_match_share\n",
    "test['rfidf_share'] = pool_apply(test)\n",
    "\n",
    "test['char_difference'] = abs(test['q1_char_num'] - test['q2_char_num'])\n",
    "test['word_difference'] = abs(test['q1_word_num'] - test['q2_word_num'])\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "test['seq_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "test['shingle_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_seq\n",
    "test['seq_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "apply_func = simhash_distance_shingle\n",
    "test['shingle_simhash_distance'] = pool_apply(test)\n",
    "\n",
    "test['avg_word_len_q1'] = test['q1_char_num'] / (test['q1_word_num'] + 10e-4)\n",
    "test['avg_word_len_q2'] = test['q2_char_num'] / (test['q2_word_num'] + 10e-4)\n",
    "test['avg_word_difference'] = abs(test['avg_word_len_q1'] - test['avg_word_len_q2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>word_share</th>\n",
       "      <th>start_with_same_world</th>\n",
       "      <th>q1_char_num</th>\n",
       "      <th>q2_char_num</th>\n",
       "      <th>q1_word_num</th>\n",
       "      <th>q2_word_num</th>\n",
       "      <th>rfidf_share</th>\n",
       "      <th>char_difference</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>seq_simhash_distance</th>\n",
       "      <th>shingle_simhash_distance</th>\n",
       "      <th>avg_word_len_q1</th>\n",
       "      <th>avg_word_len_q2</th>\n",
       "      <th>avg_word_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.772164</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>4.713949</td>\n",
       "      <td>4.749604</td>\n",
       "      <td>0.035655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0.361758</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>6.374203</td>\n",
       "      <td>6.768710</td>\n",
       "      <td>0.394507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0.355191</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>5.213913</td>\n",
       "      <td>5.899410</td>\n",
       "      <td>0.685497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>4.545041</td>\n",
       "      <td>7.221420</td>\n",
       "      <td>2.676378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>5.845704</td>\n",
       "      <td>5.570633</td>\n",
       "      <td>0.275071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "   word_share  start_with_same_world  q1_char_num  q2_char_num  q1_word_num  \\\n",
       "0    0.727273                      1           66           57           14   \n",
       "1    0.307692                      1           51           88            8   \n",
       "2    0.363636                      1           73           59           14   \n",
       "3    0.000000                      0           50           65           11   \n",
       "4    0.000000                      1           76           39           13   \n",
       "\n",
       "   q2_word_num  rfidf_share  char_difference  word_difference  \\\n",
       "0           12     0.772164                9                2   \n",
       "1           13     0.361758               37                5   \n",
       "2           10     0.355191               14                4   \n",
       "3            9     0.000000               15                2   \n",
       "4            7     0.000000               37                6   \n",
       "\n",
       "   seq_simhash_distance  shingle_simhash_distance  avg_word_len_q1  \\\n",
       "0                    15                        10         4.713949   \n",
       "1                    22                        18         6.374203   \n",
       "2                    26                        23         5.213913   \n",
       "3                    36                        28         4.545041   \n",
       "4                    34                        21         5.845704   \n",
       "\n",
       "   avg_word_len_q2  avg_word_difference  \n",
       "0         4.749604             0.035655  \n",
       "1         6.768710             0.394507  \n",
       "2         5.899410             0.685497  \n",
       "3         7.221420             2.676378  \n",
       "4         5.570633             0.275071  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>start_with_same_world</th>\n",
       "      <th>word_share</th>\n",
       "      <th>q1_char_num</th>\n",
       "      <th>q2_char_num</th>\n",
       "      <th>q1_word_num</th>\n",
       "      <th>q2_word_num</th>\n",
       "      <th>rfidf_share</th>\n",
       "      <th>char_difference</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>seq_simhash_distance</th>\n",
       "      <th>shingle_simhash_distance</th>\n",
       "      <th>avg_word_len_q1</th>\n",
       "      <th>avg_word_len_q2</th>\n",
       "      <th>avg_word_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>0.274019</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>5.181347</td>\n",
       "      <td>4.856796</td>\n",
       "      <td>0.324551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>66</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>0.480962</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>4.713949</td>\n",
       "      <td>6.141980</td>\n",
       "      <td>1.428031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0.468893</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>4.285408</td>\n",
       "      <td>4.832528</td>\n",
       "      <td>0.547120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>6.748313</td>\n",
       "      <td>5.664778</td>\n",
       "      <td>1.083535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>7.998000</td>\n",
       "      <td>4.999167</td>\n",
       "      <td>2.998834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  start_with_same_world  \\\n",
       "0  Why did Microsoft choose core m3 and not core ...                      0   \n",
       "1        How much cost does hair transplant require?                      0   \n",
       "2                      What you send money to China?                      1   \n",
       "3                                  What foods fibre?                      0   \n",
       "4                     How their can I start reading?                      1   \n",
       "\n",
       "   word_share  q1_char_num  q2_char_num  q1_word_num  q2_word_num  \\\n",
       "0    0.266667           57           68           11           14   \n",
       "1    0.500000           66           43           14            7   \n",
       "2    0.444444           60           29           14            6   \n",
       "3    0.000000           27           17            4            3   \n",
       "4    0.800000           32           30            4            6   \n",
       "\n",
       "   rfidf_share  char_difference  word_difference  seq_simhash_distance  \\\n",
       "0     0.274019               11                3                    26   \n",
       "1     0.480962               23                7                    27   \n",
       "2     0.468893               31                8                    22   \n",
       "3     0.000000               10                1                    32   \n",
       "4     1.000000                2                2                    21   \n",
       "\n",
       "   shingle_simhash_distance  avg_word_len_q1  avg_word_len_q2  \\\n",
       "0                        23         5.181347         4.856796   \n",
       "1                        22         4.713949         6.141980   \n",
       "2                        27         4.285408         4.832528   \n",
       "3                        33         6.748313         5.664778   \n",
       "4                        23         7.998000         4.999167   \n",
       "\n",
       "   avg_word_difference  \n",
       "0             0.324551  \n",
       "1             1.428031  \n",
       "2             0.547120  \n",
       "3             1.083535  \n",
       "4             2.998834  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus.reader.wordnet import ADJ, ADJ_SAT, ADV, NOUN, VERB\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for col in ['question1', 'question2']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "            \n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(train_clean)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.97593927e-01,   1.75517535e+00,  -4.31919003e+00,\n",
       "         1.08534008e-01,   1.68598020e+00,  -3.62051368e-01,\n",
       "         1.86311984e+00,  -4.87518609e-01,   2.85955048e+00,\n",
       "         1.13616097e+00,  -2.13530636e+00,  -1.16825187e+00,\n",
       "        -1.60387611e+00,  -3.93496871e-01,   1.75999808e+00,\n",
       "         5.22271454e-01,  -2.40471983e+00,   2.21439385e+00,\n",
       "        -2.61388755e+00,   2.83126307e+00,  -1.78746021e+00,\n",
       "         5.84509552e-01,  -1.22568488e+00,  -1.98163942e-01,\n",
       "         4.99132991e-01,  -2.16789961e+00,   1.09634055e-02,\n",
       "         3.29084706e+00,   4.61286688e+00,   3.31711149e+00,\n",
       "         5.24448156e-01,   4.39081907e-01,  -3.83236259e-01,\n",
       "         1.32586098e+00,   1.84228599e+00,  -1.39619267e+00,\n",
       "        -1.76345778e+00,   8.27626735e-02,  -2.29250908e+00,\n",
       "         2.59695101e+00,  -2.36521769e+00,  -1.91587949e+00,\n",
       "         7.99712956e-01,   1.12129474e+00,  -3.13834548e-01,\n",
       "         4.42366123e-01,   2.84562618e-01,  -9.27180529e-01,\n",
       "         1.15986729e+00,  -3.42564249e+00,  -1.34578812e+00,\n",
       "        -1.12473059e+00,   4.08790398e+00,  -4.47326386e-03,\n",
       "         1.29989493e+00,  -7.81608224e-01,   1.41568875e+00,\n",
       "        -1.04028010e+00,  -3.96272278e+00,   2.30483007e+00,\n",
       "        -8.84578407e-01,   9.53240573e-01,  -1.40816331e+00,\n",
       "         2.03521156e+00,   2.48257446e+00,  -1.32040691e+00,\n",
       "         3.34294271e+00,   1.01659632e+00,   3.61738093e-02,\n",
       "        -2.52244800e-01,  -1.08058918e+00,   7.17900038e-01,\n",
       "         5.01317739e-01,   9.46861744e-01,   1.19813979e+00,\n",
       "        -3.83897042e+00,  -3.52928162e-01,  -1.73781896e+00,\n",
       "        -1.26487577e+00,   3.11584353e+00,   1.50777829e+00,\n",
       "         1.26194251e+00,  -9.45373178e-01,   1.79592729e-01,\n",
       "         2.96987796e+00,   1.94623423e+00,  -1.67504251e-01,\n",
       "         3.18904787e-01,  -1.59260511e+00,   2.55383992e+00,\n",
       "         1.11521649e+00,  -1.09007275e+00,  -9.53958511e-01,\n",
       "         1.29886955e-01,   1.54392943e-01,  -3.03038955e-01,\n",
       "        -2.60130614e-01,  -1.80854893e+00,   2.09551072e+00,\n",
       "         2.96181250e+00], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4)\n",
    "word2vec_model.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv['question']\n",
    "\n",
    "'question' in word2vec_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def question_to_word2vec(question_string, word2vec_model):\n",
    "    \"\"\"\n",
    "    Given question string, returns word2vec vector of the questions tring\n",
    "    :param question_string : The given question as a string.\n",
    "    \"\"\"\n",
    "    stops_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    if not isinstance(question_string, str):\n",
    "        return 0\n",
    "    \n",
    "    words = word_tokenize(question_string)[:-1]\n",
    "    non_stop_words = []\n",
    "    for word in words:\n",
    "        if word.lower().strip('-') not in stops_words:\n",
    "            word = WordNetLemmatizer().lemmatize(word, NOUN)\n",
    "            \n",
    "            if word.lower() in word2vec_model.wv:\n",
    "                non_stop_words.append(word.lower().strip('-'))\n",
    "            \n",
    "    if len(non_stop_words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    vectors = [word2vec_model.wv[word] for word in non_stop_words]\n",
    "    vector = sum(vectors)/float(len(non_stop_words))\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def numpy_cosine(row):\n",
    "    \"\"\"\n",
    "    Cosine similarity between q1 and q2 question instances using their vectors\n",
    "    :return: similarity between q1 and q2\n",
    "    \"\"\"\n",
    "    q1, q2 = row['question1'], row['question2']\n",
    "    q1_vec, q2_vec = question_to_word2vec(q1, word2vec_model), question_to_word2vec(q2, word2vec_model)\n",
    "    \n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cosine_similarity = np.dot(q1_vec, q2_vec) / (np.linalg.norm(q1_vec) * np.linalg.norm(q2_vec))\n",
    "    \n",
    "    return cosine_similarity if isinstance(cosine_similarity, np.float32) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "apply_func = numpy_cosine\n",
    "train['cosin_sim'] = pool_apply(train, proc_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apply_func = numpy_cosine\n",
    "test['cosin_sim'] = pool_apply(test, proc_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>word_share</th>\n",
       "      <th>start_with_same_world</th>\n",
       "      <th>q1_char_num</th>\n",
       "      <th>q2_char_num</th>\n",
       "      <th>...</th>\n",
       "      <th>q2_word_num</th>\n",
       "      <th>rfidf_share</th>\n",
       "      <th>char_difference</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>seq_simhash_distance</th>\n",
       "      <th>shingle_simhash_distance</th>\n",
       "      <th>avg_word_len_q1</th>\n",
       "      <th>avg_word_len_q2</th>\n",
       "      <th>avg_word_difference</th>\n",
       "      <th>cosin_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>How do we prepare for UPSC?</td>\n",
       "      <td>How do I prepare for civil service?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.328275</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>4.499250</td>\n",
       "      <td>4.999286</td>\n",
       "      <td>0.500036</td>\n",
       "      <td>0.790161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>What is the stall speed and AOA of an f-14 wit...</td>\n",
       "      <td>Why did aircraft stop using variable-sweep win...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>4.733018</td>\n",
       "      <td>5.999500</td>\n",
       "      <td>1.266482</td>\n",
       "      <td>0.310878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>Why do Slavs squat?</td>\n",
       "      <td>Will squats make my legs thicker?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>4.748813</td>\n",
       "      <td>5.499083</td>\n",
       "      <td>0.750271</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  qid1  qid2                                          question1  \\\n",
       "38  38    77    78                        How do we prepare for UPSC?   \n",
       "39  39    79    80  What is the stall speed and AOA of an f-14 wit...   \n",
       "40  40    81    82                                Why do Slavs squat?   \n",
       "\n",
       "                                            question2  is_duplicate  \\\n",
       "38                How do I prepare for civil service?             1   \n",
       "39  Why did aircraft stop using variable-sweep win...             0   \n",
       "40                  Will squats make my legs thicker?             0   \n",
       "\n",
       "    word_share  start_with_same_world  q1_char_num  q2_char_num    ...      \\\n",
       "38         0.4                      1           27           35    ...       \n",
       "39         0.0                      0           71           72    ...       \n",
       "40         0.0                      0           19           33    ...       \n",
       "\n",
       "    q2_word_num  rfidf_share  char_difference  word_difference  \\\n",
       "38            7     0.328275                8                1   \n",
       "39           12     0.000000                1                3   \n",
       "40            6     0.000000               14                2   \n",
       "\n",
       "    seq_simhash_distance  shingle_simhash_distance  avg_word_len_q1  \\\n",
       "38                    20                        21         4.499250   \n",
       "39                    42                        35         4.733018   \n",
       "40                    21                        33         4.748813   \n",
       "\n",
       "    avg_word_len_q2  avg_word_difference  cosin_sim  \n",
       "38         4.999286             0.500036   0.790161  \n",
       "39         5.999500             1.266482   0.310878  \n",
       "40         5.499083             0.750271   0.000000  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[38:41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature picking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#features = ['cosin_sim', 'word_share', 'q1_char_num', 'q1_word_num', 'q2_char_num', 'q2_word_num',\n",
    "#            'start_with_same_world', 'rfidf_share']\n",
    "\n",
    "features = ['cosin_sim', 'word_share', 'q1_char_num', 'q1_word_num', 'q2_char_num', 'q2_word_num',\n",
    "            'start_with_same_world', 'rfidf_share', 'char_difference', 'word_difference',\n",
    "           'seq_simhash_distance', 'shingle_simhash_distance', 'avg_word_len_q1', 'avg_word_len_q2',\n",
    "           'avg_word_difference']\n",
    "\n",
    "#features = ['cosin_sim', 'start_with_same_world', 'rfidf_share']\n",
    "\n",
    "target = 'is_duplicate'\n",
    "\n",
    "X = train[features]\n",
    "y = train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19124366100096607\n"
     ]
    }
   ],
   "source": [
    "pos_train = X[y == 1]\n",
    "neg_train = X[y == 0]\n",
    "\n",
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "X = pd.concat([pos_train, neg_train])\n",
    "y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_vald, y_train, y_vald = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosin_sim</th>\n",
       "      <th>word_share</th>\n",
       "      <th>q1_char_num</th>\n",
       "      <th>q1_word_num</th>\n",
       "      <th>q2_char_num</th>\n",
       "      <th>q2_word_num</th>\n",
       "      <th>start_with_same_world</th>\n",
       "      <th>rfidf_share</th>\n",
       "      <th>char_difference</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>seq_simhash_distance</th>\n",
       "      <th>shingle_simhash_distance</th>\n",
       "      <th>avg_word_len_q1</th>\n",
       "      <th>avg_word_len_q2</th>\n",
       "      <th>avg_word_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.686881</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.274019</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>5.181347</td>\n",
       "      <td>4.856796</td>\n",
       "      <td>0.324551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.741746</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.480962</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>4.713949</td>\n",
       "      <td>6.141980</td>\n",
       "      <td>1.428031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.837249</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.468893</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>4.285408</td>\n",
       "      <td>4.832528</td>\n",
       "      <td>0.547120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>6.748313</td>\n",
       "      <td>5.664778</td>\n",
       "      <td>1.083535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>7.998000</td>\n",
       "      <td>4.999167</td>\n",
       "      <td>2.998834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cosin_sim  word_share  q1_char_num  q1_word_num  q2_char_num  q2_word_num  \\\n",
       "0   0.686881    0.266667           57           11           68           14   \n",
       "1   0.741746    0.500000           66           14           43            7   \n",
       "2   0.837249    0.444444           60           14           29            6   \n",
       "3   1.000000    0.000000           27            4           17            3   \n",
       "4   1.000000    0.800000           32            4           30            6   \n",
       "\n",
       "   start_with_same_world  rfidf_share  char_difference  word_difference  \\\n",
       "0                      0     0.274019               11                3   \n",
       "1                      0     0.480962               23                7   \n",
       "2                      1     0.468893               31                8   \n",
       "3                      0     0.000000               10                1   \n",
       "4                      1     1.000000                2                2   \n",
       "\n",
       "   seq_simhash_distance  shingle_simhash_distance  avg_word_len_q1  \\\n",
       "0                    26                        23         5.181347   \n",
       "1                    27                        22         4.713949   \n",
       "2                    22                        27         4.285408   \n",
       "3                    32                        33         6.748313   \n",
       "4                    21                        23         7.998000   \n",
       "\n",
       "   avg_word_len_q2  avg_word_difference  \n",
       "0         4.856796             0.324551  \n",
       "1         6.141980             1.428031  \n",
       "2         4.832528             0.547120  \n",
       "3         5.664778             1.083535  \n",
       "4         4.999167             2.998834  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test[features]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Transofmrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Model works fine without scaling\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_vald_scaled = scaler.transform(X_vald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, log_loss\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "#import lightgbm\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=87, n_jobs=8)   # 0.39680 (on public)\n",
    "#model = ExtraTreesClassifier(n_estimators=62, n_jobs=8) # 0.48183 (on public)\n",
    "#model = AdaBoostClassifier()\n",
    "model = GradientBoostingClassifier(n_estimators=500, max_depth=4, learning_rate=0.2, subsample=0.7) # 0.34721 (on public)\n",
    "#model = KNeighborsClassifier(n_neighbors=25)\n",
    "#model = MultinomialNB() # 0.57\n",
    "#model = SVC()\n",
    "\n",
    "#model = XGBClassifier(n_estimators=500, learning_rate=0.2, max_depth=4, subsample=0.7, gamma=0.5, seed=42,\n",
    "#            colsample_bytree=0.7) # 0.34785 (on public)\n",
    "#model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_vald, y_vald)],\n",
    "#          early_stopping_rounds=50, verbose=True, eval_metric='logloss')\n",
    "\n",
    "\n",
    "#model = VotingClassifier(estimators=[('xgb', xgb), ('knn', knn), ('rf', rf)],\n",
    "#                        voting='soft', weights=[4.5, 1.1, 1.2])\n",
    "\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_predictions = model.predict(X_vald)\n",
    "val_prob_predictions = model.predict_proba(X_vald)\n",
    "\n",
    "for metric_name, metric_func in zip(\n",
    "    ['F1-score', 'Acc', 'Precision', 'Recall', 'LogLoss'],\n",
    "    [f1_score, accuracy_score, precision_score, recall_score, log_loss]\n",
    "):\n",
    "    \n",
    "    val_predictions = val_predictions if metric_name not in ['LogLoss'] else val_prob_predictions\n",
    "    metric_score = metric_func(y_vald, val_predictions)\n",
    "    print('{0}: {1}'.format(metric_name, metric_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.2, loss='deviance',\n",
       "              max_depth=4, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              presort='auto', random_state=None, subsample=0.7, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del X, y, X_train, y_train, X_vald, y_vald\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    'submission.csv', np.c_[range(len(predictions)), predictions[:,1]],\n",
    "    delimiter=',', header='test_id,is_duplicate', comments='', fmt='%d,%f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w', buffering=1) as submission_file:\n",
    "    submission_file.write('test_id,is_duplicate')\n",
    "    \n",
    "    for test_id, test_row in enumerate(X_test.iterrows()):\n",
    "        row_prediction = model.predict_proba(X_test[test_id:])\n",
    "        submission_file.write('%d,%f' % test_id, row_prediction[:,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
